{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from text_utils import *\n",
    "\n",
    "#current version\n",
    "#export TF_BINARY_URL=https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow_gpu-1.0.1-cp36-cp36m-linux_x86_64.whl\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn.python.ops import core_rnn_cell as rnn_cell\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn,dynamic_rnn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import *\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from os import walk\n",
    "import h5py\n",
    "import sys\n",
    "sys.path.append('../tfmodels')\n",
    "#from tf_object import *\n",
    "from text_cnn import *\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsentence_ids = []\\nlabel = []\\nid_term_map = {}\\nfor idx, file in enumerate(['neg','pos']):\\n    sentences = []\\n    file_name = 'sentiment/{}.txt'.format(file)\\n    with open(file_name,'r') as f:\\n        print(file_name, len(id_term_map))\\n        for line in f:\\n            sentence_ids.append(update_bow(weibo_filter(line.strip()), id_term_map, terminator=0))\\n            label.append(idx)\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sentence_ids = []\n",
    "label = []\n",
    "id_term_map = {}\n",
    "for idx, file in enumerate(['neg','pos']):\n",
    "    sentences = []\n",
    "    file_name = 'sentiment/{}.txt'.format(file)\n",
    "    with open(file_name,'r') as f:\n",
    "        print(file_name, len(id_term_map))\n",
    "        for line in f:\n",
    "            sentence_ids.append(update_bow(weibo_filter(line.strip()), id_term_map, terminator=0))\n",
    "            label.append(idx)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsentence = []\\nlabel = []\\nfor (dirpath, dirnames, filenames) in walk('my_data/labeled_data/'):\\n    for file in filenames:\\n        word_df = pd.read_excel('my_data/labeled_data/{}'.format(file))\\n        sentence += word_df['content'].tolist()\\n        label += word_df['label'].tolist()\\n\\nsentence_ids = []\\nid_term_map = {}\\n\\nlabel_dict = {'中性':0,'正面':1,'负面':2, '无关':3}  \\nlabel = [label_dict[i] for i in label]\\nfor line in sentence:\\n    sentence_ids.append(update_bow(weibo_filter(line.strip()), id_term_map, terminator=0))\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sentence = []\n",
    "label = []\n",
    "for (dirpath, dirnames, filenames) in walk('my_data/labeled_data/'):\n",
    "    for file in filenames:\n",
    "        word_df = pd.read_excel('my_data/labeled_data/{}'.format(file))\n",
    "        sentence += word_df['content'].tolist()\n",
    "        label += word_df['label'].tolist()\n",
    "\n",
    "sentence_ids = []\n",
    "id_term_map = {}\n",
    "\n",
    "label_dict = {'中性':0,'正面':1,'负面':2, '无关':3}  \n",
    "label = [label_dict[i] for i in label]\n",
    "for line in sentence:\n",
    "    sentence_ids.append(update_bow(weibo_filter(line.strip()), id_term_map, terminator=0))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f = open('emotion_sents.dat','rb')\n",
    "new_sentences = pkl.load(f)\n",
    "label_str = pkl.load(f)\n",
    "f.close()\n",
    "df = pd.read_excel('emotion.xlsx')\n",
    "maxlen = 140\n",
    "phrase_dict = dict(zip(df.phrase, df.po1))\n",
    "id_term_map = pkl.load(open('term_map.dat','rb'))\n",
    "term_dict = dict((v,k) for k,v in id_term_map.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n",
      "5000000\n",
      "5100000\n",
      "5200000\n",
      "5300000\n",
      "5400000\n",
      "5500000\n",
      "5600000\n",
      "5700000\n",
      "5800000\n",
      "5900000\n",
      "6000000\n",
      "6100000\n",
      "6200000\n",
      "6300000\n",
      "6400000\n",
      "6500000\n",
      "6600000\n",
      "6700000\n",
      "6800000\n",
      "6900000\n",
      "7000000\n",
      "7100000\n",
      "7200000\n",
      "7300000\n",
      "7400000\n",
      "7500000\n",
      "7600000\n",
      "7700000\n",
      "7800000\n"
     ]
    }
   ],
   "source": [
    "#label_dict = {x:i for i,x in enumerate(set(label))}\n",
    "label_dict = dict(zip(df.phrase,df.po2))\n",
    "label = [label_dict[i] for i in label_str]\n",
    "\n",
    "sentence_ids = []\n",
    "id_term_map = {}\n",
    "for line in new_sentences:\n",
    "    sentence_ids.append(update_bow(weibo_filter(line.strip()), id_term_map, terminator=0))\n",
    "pkl.dump(id_term_map, open('term_map.dat','wb'))\n",
    "\n",
    "#seq_len = np.array([len(s)-1 for s in sentence_ids]) \n",
    "#new_idx = np.arange(0,len(seq_len))[np.logical_and(seq_len<maxlen, seq_len>0)]\n",
    "seq_len = np.array([len(s) for s in sentence_ids]) \n",
    "new_idx = np.arange(0,len(seq_len))[seq_len<maxlen]\n",
    "seq_len = seq_len[new_idx]\n",
    "label = np.array(label)[new_idx]\n",
    "documents = np.empty(shape=(len(seq_len),maxlen))\n",
    "for i, idx in enumerate(new_idx):\n",
    "    if i%100000==0:\n",
    "        print(i)\n",
    "    documents[i] = np.pad(np.array(sentence_ids[idx]),((0),(maxlen-seq_len[i])), mode='constant', constant_values=0)  \n",
    "label += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_path = 'emotion_documents'\n",
    "h5f = h5py.File(file_path + '.h5', 'w')\n",
    "h5f.create_dataset('documents', data=documents)\n",
    "h5f.create_dataset('seq_len', data=seq_len)\n",
    "h5f.create_dataset('label', data=label)\n",
    "h5f.create_dataset('new_idx', data=new_idx)\n",
    "h5f.create_dataset('id_term_map_length', data=len(id_term_map))\n",
    "h5f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_path = 'emotion_documents'\n",
    "h5f = h5py.File(file_path + '.h5', 'r')\n",
    "documents = np.array(h5f['documents'])\n",
    "seq_len = np.array(h5f['seq_len'])\n",
    "label = np.array(h5f['label'])\n",
    "new_idx = np.array(h5f['new_idx'])\n",
    "nb_words = np.array(h5f['id_term_map_length'])\n",
    "h5f.close()\n",
    "label_str = [label_str[idx] for idx in new_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 474546, 1: 2084242, 2: 1380949, 3: 2024276, 4: 1846890})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_integer(\"nb_words\", nb_words, \"term number in input sequence(zero mask) [20001]\")\n",
    "flags.DEFINE_integer(\"maxlen\", maxlen, \"the max length of input sequence [80]\")\n",
    "flags.DEFINE_integer(\"embedding_size\", 300, \"word embedding size [50]\")\n",
    "flags.DEFINE_float(\"init_scale\", 1.0, \"init scale for embedding layer\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.001, \"learning rate [0.001]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 512, \"batch size to use during training [128]\")\n",
    "flags.DEFINE_float(\"clip_gradients\", 5.0, \"clip gradients to this norm [5.0]\")\n",
    "flags.DEFINE_integer(\"n_epochs\", 50, \"number of epoch to use during training [10]\")\n",
    "flags.DEFINE_boolean(\"epoch_save\", True, \"save checkpoint or not in each epoch [True]\")\n",
    "flags.DEFINE_integer(\"print_step\", 100, \"print step duraing training [100]\")\n",
    "flags.DEFINE_string(\"logs_dir\", \"logs/\", \"logs directory [logs/]\")\n",
    "flags.DEFINE_string(\"model_dir\", \"model/\", \"model directory [model/]\")\n",
    "flags.DEFINE_boolean(\"dir_clear\", False, \"clear the log and model directory\")\n",
    "flags.DEFINE_boolean(\"lr_annealing\", False, \"use lr annealing or not after each epoch [False]\")\n",
    "flags.DEFINE_string(\"current_task_name\", 'url_self_prediction', \"current task name [self_prediction]\")\n",
    "flags.DEFINE_integer(\"gpu_id\", 0, \"default gpu id [0]\")\n",
    "flags.DEFINE_integer(\"gpu_num\", 4, \"gpu_num\")\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "idxs = np.arange(0, len(seq_len))\n",
    "label[label<=1] = 0\n",
    "label[label>=3] = 1\n",
    "#2330054\n",
    "idxs = np.append(np.random.choice(np.where(label == 0)[0], 437659, replace=False),\n",
    "                 np.random.choice(np.where(label == 1)[0], 437659, replace=False))\n",
    "#idxs = np.delete(idxs, np.where(label == 2)[0])\n",
    "#idxs = np.delete(idxs, np.where(label == 3)[0])\n",
    "#idxs = idxs[0:200000]\n",
    "train_idxs, test_idxs = train_test_split(idxs, test_size=0.2, random_state=42)\n",
    "test_idxs = np.sort(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>variable_shape</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>global/Variable:0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpu/embedding/embedding_layer/embedding_table:0</td>\n",
       "      <td>[15987, 300]</td>\n",
       "      <td>4796100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpu/conv/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[7, 300, 256]</td>\n",
       "      <td>537600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpu/conv/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpu/conv_1/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[7, 256, 256]</td>\n",
       "      <td>458752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpu/conv_1/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpu/conv_2/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[3, 256, 256]</td>\n",
       "      <td>196608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpu/conv_2/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpu/conv_3/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[3, 256, 256]</td>\n",
       "      <td>196608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpu/conv_3/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpu/conv_4/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[3, 256, 256]</td>\n",
       "      <td>196608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpu/conv_4/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpu/conv_5/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[3, 256, 256]</td>\n",
       "      <td>196608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpu/conv_5/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpu/fully_connected/W:0</td>\n",
       "      <td>[256, 1024]</td>\n",
       "      <td>262144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpu/fully_connected/B:0</td>\n",
       "      <td>[1024]</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpu/fully_connected_1/W:0</td>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>1048576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpu/fully_connected_1/B:0</td>\n",
       "      <td>[1024]</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpu/fully_connected_2/W:0</td>\n",
       "      <td>[1024, 2]</td>\n",
       "      <td>2048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpu/fully_connected_2/B:0</td>\n",
       "      <td>[2]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      variable_name variable_shape  parameters\n",
       "0                                 global/Variable:0             []         1.0\n",
       "1   gpu/embedding/embedding_layer/embedding_table:0   [15987, 300]   4796100.0\n",
       "2                   gpu/conv/conv_1d/he_uniform/W:0  [7, 300, 256]    537600.0\n",
       "3                              gpu/conv/conv_1d/B:0          [256]       256.0\n",
       "4                 gpu/conv_1/conv_1d/he_uniform/W:0  [7, 256, 256]    458752.0\n",
       "5                            gpu/conv_1/conv_1d/B:0          [256]       256.0\n",
       "6                 gpu/conv_2/conv_1d/he_uniform/W:0  [3, 256, 256]    196608.0\n",
       "7                            gpu/conv_2/conv_1d/B:0          [256]       256.0\n",
       "8                 gpu/conv_3/conv_1d/he_uniform/W:0  [3, 256, 256]    196608.0\n",
       "9                            gpu/conv_3/conv_1d/B:0          [256]       256.0\n",
       "10                gpu/conv_4/conv_1d/he_uniform/W:0  [3, 256, 256]    196608.0\n",
       "11                           gpu/conv_4/conv_1d/B:0          [256]       256.0\n",
       "12                gpu/conv_5/conv_1d/he_uniform/W:0  [3, 256, 256]    196608.0\n",
       "13                           gpu/conv_5/conv_1d/B:0          [256]       256.0\n",
       "14                          gpu/fully_connected/W:0    [256, 1024]    262144.0\n",
       "15                          gpu/fully_connected/B:0         [1024]      1024.0\n",
       "16                        gpu/fully_connected_1/W:0   [1024, 1024]   1048576.0\n",
       "17                        gpu/fully_connected_1/B:0         [1024]      1024.0\n",
       "18                        gpu/fully_connected_2/W:0      [1024, 2]      2048.0\n",
       "19                        gpu/fully_connected_2/B:0            [2]         2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 ... training ...\n",
      "Minibatch 100 / loss: 0.698411\n",
      "Minibatch 100 / accuracy: 0.474609\n",
      "Minibatch 200 / loss: 0.695136\n",
      "Minibatch 200 / accuracy: 0.494141\n",
      "Minibatch 300 / loss: 0.69388\n",
      "Minibatch 300 / accuracy: 0.478516\n",
      "Minibatch 400 / loss: 0.693801\n",
      "Minibatch 400 / accuracy: 0.505859\n",
      "Minibatch 500 / loss: 0.694499\n",
      "Minibatch 500 / accuracy: 0.490234\n",
      "Minibatch 600 / loss: 0.691916\n",
      "Minibatch 600 / accuracy: 0.53125\n",
      "Minibatch 700 / loss: 0.695897\n",
      "Minibatch 700 / accuracy: 0.492188\n",
      "Minibatch 800 / loss: 0.693354\n",
      "Minibatch 800 / accuracy: 0.498047\n",
      "Minibatch 900 / loss: 0.69398\n",
      "Minibatch 900 / accuracy: 0.488281\n",
      "Minibatch 1000 / loss: 0.697048\n",
      "Minibatch 1000 / accuracy: 0.462891\n",
      "Minibatch 1100 / loss: 0.697587\n",
      "Minibatch 1100 / accuracy: 0.478516\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a865b5ceaec4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/ceph/jenkins/wei.wang/tftools/tf_object.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, input_list, train_idxs, test_idxs, run_type, shuffle)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'... training ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mepochLoss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochMetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_idxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'epoch time:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/ceph/jenkins/wei.wang/tftools/tf_object.py\u001b[0m in \u001b[0;36mmodel_run\u001b[0;34m(self, input_list, idxs, run_type, mode, shuffle, save_metric)\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;31m#print('model training...')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_step\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_step\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_prediction'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'{}_prediction'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeedDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/CORP/jenkins/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/CORP/jenkins/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/CORP/jenkins/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/CORP/jenkins/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/CORP/jenkins/.local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph_to_use = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(graph=graph_to_use, config=config) as session:\n",
    "    #cnn_model = TextCNN(FLAGS, session, current_task_name='text_cnn_model')\n",
    "    #cnn_model.build_model(num_classes=len(set(label[idxs])),max_conv_len=7, num_filters=512, dropout_keep_prob=0.5)\n",
    "    cnn_model = TextMultiCNN(FLAGS, session, current_task_name='text_multi_cnn_model')\n",
    "    #label[idxs]\n",
    "    cnn_model.build_model(num_classes = len(set(label[idxs])), num_filters_per_size=256, fully_layers = [1024, 1024])\n",
    "    cnn_model.build_model_summary()\n",
    "    display(cnn_model.model_summary())\n",
    "    cnn_model.run([documents,label], train_idxs, test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "INFO:tensorflow:Restoring parameters from model/text_multi_cnn_model.ckpt\n",
      "Load Model ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>variable_shape</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>global/Variable:0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpu/embedding/embedding_layer/embedding_table:0</td>\n",
       "      <td>[15987, 300]</td>\n",
       "      <td>4796100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpu/conv/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[7, 300, 256]</td>\n",
       "      <td>537600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpu/conv/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpu/conv_1/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[7, 256, 256]</td>\n",
       "      <td>458752.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpu/conv_1/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpu/conv_2/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[3, 256, 256]</td>\n",
       "      <td>196608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpu/conv_2/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpu/conv_3/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[3, 256, 256]</td>\n",
       "      <td>196608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpu/conv_3/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpu/conv_4/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[3, 256, 256]</td>\n",
       "      <td>196608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>gpu/conv_4/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>gpu/conv_5/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[3, 256, 256]</td>\n",
       "      <td>196608.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>gpu/conv_5/conv_1d/B:0</td>\n",
       "      <td>[256]</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>gpu/fully_connected/W:0</td>\n",
       "      <td>[256, 1024]</td>\n",
       "      <td>262144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gpu/fully_connected/B:0</td>\n",
       "      <td>[1024]</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>gpu/fully_connected_1/W:0</td>\n",
       "      <td>[1024, 1024]</td>\n",
       "      <td>1048576.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>gpu/fully_connected_1/B:0</td>\n",
       "      <td>[1024]</td>\n",
       "      <td>1024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>gpu/fully_connected_2/W:0</td>\n",
       "      <td>[1024, 2]</td>\n",
       "      <td>2048.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>gpu/fully_connected_2/B:0</td>\n",
       "      <td>[2]</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      variable_name variable_shape  parameters\n",
       "0                                 global/Variable:0             []         1.0\n",
       "1   gpu/embedding/embedding_layer/embedding_table:0   [15987, 300]   4796100.0\n",
       "2                   gpu/conv/conv_1d/he_uniform/W:0  [7, 300, 256]    537600.0\n",
       "3                              gpu/conv/conv_1d/B:0          [256]       256.0\n",
       "4                 gpu/conv_1/conv_1d/he_uniform/W:0  [7, 256, 256]    458752.0\n",
       "5                            gpu/conv_1/conv_1d/B:0          [256]       256.0\n",
       "6                 gpu/conv_2/conv_1d/he_uniform/W:0  [3, 256, 256]    196608.0\n",
       "7                            gpu/conv_2/conv_1d/B:0          [256]       256.0\n",
       "8                 gpu/conv_3/conv_1d/he_uniform/W:0  [3, 256, 256]    196608.0\n",
       "9                            gpu/conv_3/conv_1d/B:0          [256]       256.0\n",
       "10                gpu/conv_4/conv_1d/he_uniform/W:0  [3, 256, 256]    196608.0\n",
       "11                           gpu/conv_4/conv_1d/B:0          [256]       256.0\n",
       "12                gpu/conv_5/conv_1d/he_uniform/W:0  [3, 256, 256]    196608.0\n",
       "13                           gpu/conv_5/conv_1d/B:0          [256]       256.0\n",
       "14                          gpu/fully_connected/W:0    [256, 1024]    262144.0\n",
       "15                          gpu/fully_connected/B:0         [1024]      1024.0\n",
       "16                        gpu/fully_connected_1/W:0   [1024, 1024]   1048576.0\n",
       "17                        gpu/fully_connected_1/B:0         [1024]      1024.0\n",
       "18                        gpu/fully_connected_2/W:0      [1024, 2]      2048.0\n",
       "19                        gpu/fully_connected_2/B:0            [2]         2.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch 1000 / loss: 0.197946\n",
      "Minibatch 1000 / accuracy: 0.943359\n"
     ]
    }
   ],
   "source": [
    "graph_to_use = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(graph=graph_to_use, config=config) as session:\n",
    "    cnn_model = TextMultiCNN(FLAGS, session, current_task_name='text_multi_cnn_model')\n",
    "    #label[idxs]\n",
    "    cnn_model.build_model(num_classes = len(set(label[idxs])), num_filters_per_size=256, fully_layers = [1024, 1024])\n",
    "    cnn_model.build_model_summary()\n",
    "    cnn_model.model_restore()\n",
    "    display(cnn_model.model_summary())\n",
    "    _, testMetric, results = cnn_model.model_run([documents,label], \n",
    "                                                  test_idxs, run_type='self',\n",
    "                                                 mode='test', shuffle=False, save_metric=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#mege the results and get the label_str\n",
    "results = np.concatenate(results)\n",
    "test_idxs = test_idxs[0:len(results)]\n",
    "test_label_str = [label_str[idx] for idx in test_idxs]\n",
    "test_seq_len = seq_len[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78917117130051706"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_threshold = 60\n",
    "idx = 1\n",
    "accuracy_score(label[test_idxs][np.logical_and(test_seq_len<=len_threshold,label[test_idxs]==idx)],\n",
    "               np.argmax(results,1)[np.logical_and(test_seq_len<=len_threshold,label[test_idxs]==idx)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#append the str_idx\n",
    "label_str_dict = {}\n",
    "for idx, label_str in enumerate(test_label_str):\n",
    "    if label_str not in label_str_dict:\n",
    "        label_str_dict[label_str] = []\n",
    "    label_str_dict[label_str].append(idx)\n",
    "#record the result for each \n",
    "score_dict = {}\n",
    "size_dict = {}\n",
    "for key, value in label_str_dict.items():\n",
    "    score_dict[key] = accuracy_score(label[test_idxs][np.array(value)],np.argmax(results,1)[np.array(value)])\n",
    "    size_dict[key] = len(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[good] :  正性   47935   0.853426515072494\n",
      "[哈哈] :  正性   98706   0.5982007172816243\n",
      "[爱你] :  正性   34039   0.8286671171303505\n",
      "[嘻嘻] :  正性   45441   0.6360555445522765\n",
      "[衰] :  负性   18348   0.8109875735775016\n",
      "[心] :  正性   92937   0.8795635753252203\n",
      "[doge] :  负性   121986   0.7945092059744561\n",
      "[威武] :  正性   9603   0.8233885244194522\n",
      "[泪] :  负性   47674   0.7895708352561145\n",
      "[汗] :  负性   15709   0.7906932331784328\n",
      "[偷乐] :  负性   11360   0.6108274647887324\n",
      "[挤眼] :  正性   5550   0.667027027027027\n",
      "[鼓掌] :  正性   19318   0.8493115229319805\n",
      "[拜拜] :  负性   33544   0.8834963033627474\n",
      "[酷] :  正性   13983   0.6208252878495316\n",
      "[阴险] :  负性   14039   0.6705605812379799\n",
      "[色] :  正性   11974   0.7745114414564891\n",
      "[笑哈哈] :  正性   5896   0.6648575305291723\n",
      "[喵喵] :  负性   19342   0.7433564264295316\n",
      "[吃惊] :  负性   29040   0.7079201101928375\n",
      "[赞] :  正性   46949   0.8831498008477284\n",
      "[耶] :  正性   7422   0.7890056588520614\n",
      "[挖鼻] :  负性   17079   0.8231746589378769\n",
      "[握手] :  正性   6536   0.9078947368421053\n",
      "[玫瑰] :  正性   2724   0.9218061674008811\n",
      "[鲜花] :  正性   5301   0.9088851160158461\n",
      "[笑cry] :  负性   121704   0.7791198317228686\n",
      "[给力] :  正性   12052   0.8760371722535679\n",
      "[怒] :  负性   26023   0.86481189716789\n",
      "[悲伤] :  负性   8072   0.7809712586719524\n",
      "[馋嘴] :  正性   14143   0.7957293360673124\n",
      "[生病] :  负性   9432   0.8615351993214588\n",
      "[太开心] :  正性   7377   0.7710451403009353\n",
      "[蛋糕] :  正性   2816   0.9272017045454546\n",
      "[可爱] :  正性   9227   0.7246125501246342\n",
      "[抓狂] :  负性   11240   0.7818505338078292\n",
      "[哼] :  负性   6425   0.8180544747081712\n",
      "[亲亲] :  正性   3474   0.7838226827864133\n",
      "[礼物] :  正性   2734   0.9407461594732992\n",
      "[赞啊] :  正性   2377   0.9162810265039967\n",
      "[太阳] :  正性   3985   0.857465495608532\n"
     ]
    }
   ],
   "source": [
    "for key in score_dict.keys():\n",
    "    print('{} :  {}   {}   {}'.format(key, phrase_dict[key], size_dict[key], score_dict[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "document_subset = documents[test_idxs]\n",
    "sample = 1\n",
    "label_str_list = []\n",
    "idx_list = []\n",
    "for key, value in label_str_dict.items():\n",
    "    label_str_list.extend([key]*sample)\n",
    "    idx_list.extend(np.random.choice(label_str_dict[key],sample).tolist())\n",
    "score_list = [results[:,1][idx] for idx in idx_list]\n",
    "label_list = [label[test_idxs][idx] for idx in idx_list]\n",
    "sent_list = [''.join([term_dict[i] for i in document_subset[idx] if i!=0]) for idx in idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>emotion_label</th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[good]</td>\n",
       "      <td>1</td>\n",
       "      <td>//:转发微博</td>\n",
       "      <td>0.790127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[哈哈]</td>\n",
       "      <td>1</td>\n",
       "      <td>前呼后拥的土豪也不少啊。。。//:我只敬重实干家！大忽悠则鄙视！//：哈哈哈，但总也追星。</td>\n",
       "      <td>0.716519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[爱你]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.699765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[嘻嘻]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.726079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[衰]</td>\n",
       "      <td>0</td>\n",
       "      <td>郑州：这个辖区报警后几乎不管，后来，找了保险公司，通过gps定位，通过购车辖区民警，找到车。...</td>\n",
       "      <td>0.739960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[心]</td>\n",
       "      <td>1</td>\n",
       "      <td>在剑川过年</td>\n",
       "      <td>0.217364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[doge]</td>\n",
       "      <td>0</td>\n",
       "      <td>看懂了呀，譬如这句“诚邀社会各界贤达暨海内外校友沥临盛举，共襄伟业，再创辉煌，特此公告”这两...</td>\n",
       "      <td>0.578732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[威武]</td>\n",
       "      <td>1</td>\n",
       "      <td>风华正茂之际，参军报国之时。中国军视网独家首曝中国武警部队2015年宣传片。</td>\n",
       "      <td>0.960041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[泪]</td>\n",
       "      <td>0</td>\n",
       "      <td>速度//://:下载来看！//://://://://:马下载。//:马//://:转给刚起...</td>\n",
       "      <td>0.386916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[汗]</td>\n",
       "      <td>0</td>\n",
       "      <td>屁股不错//:屁股这么复古啊</td>\n",
       "      <td>0.485089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[偷乐]</td>\n",
       "      <td>0</td>\n",
       "      <td>小时候就知道怎么讨女孩子欢心了//:呀呀的，小时候凭着这门手艺，连校花都追着喊我哥。</td>\n",
       "      <td>0.455398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[挤眼]</td>\n",
       "      <td>1</td>\n",
       "      <td>达拉斯转机去newark，看到有穿着疑似你阿国家队工作人员[傻眼]另外感觉今天跟我一样从sf...</td>\n",
       "      <td>0.033581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[鼓掌]</td>\n",
       "      <td>1</td>\n",
       "      <td>//://:纯干货，实用性很强！！！</td>\n",
       "      <td>0.939230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[拜拜]</td>\n",
       "      <td>0</td>\n",
       "      <td>省领导高度重视亲历事故现场，在第三天作出重要指示，要求要严肃追查事故原因，追究责任人，一定把...</td>\n",
       "      <td>0.068516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[酷]</td>\n",
       "      <td>1</td>\n",
       "      <td>【郎仙寨全体当家倾巢出动，为了更好地劫道而努力训练】二当家第一次正规训练，呼吸和步伐配合尚可...</td>\n",
       "      <td>0.971366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[阴险]</td>\n",
       "      <td>0</td>\n",
       "      <td>小伙伴只会强“抽”你</td>\n",
       "      <td>0.299891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[色]</td>\n",
       "      <td>1</td>\n",
       "      <td>藏有美女//:荣耀中国红已经到港，找我预购的联系两瓶起，多多益善！想代理的也联系大慧，要是帅...</td>\n",
       "      <td>0.808082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[笑哈哈]</td>\n",
       "      <td>1</td>\n",
       "      <td>爸爸和女儿用咏春对打，最后老爸得手~超有爱~</td>\n",
       "      <td>0.988662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[喵喵]</td>\n",
       "      <td>0</td>\n",
       "      <td>lcdv-40662，鷹羽澪的写真作品。//:老司机</td>\n",
       "      <td>0.351186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[吃惊]</td>\n",
       "      <td>0</td>\n",
       "      <td>我真的得喝茶静静静完好好学习哈//:普通的书都对你太简单了//:为啥呢？//:求推荐几本看看...</td>\n",
       "      <td>0.786918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[赞]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.756666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[耶]</td>\n",
       "      <td>1</td>\n",
       "      <td>xoom一下，让快乐传递到至爱手里。支持你们举行好活动转走了！</td>\n",
       "      <td>0.986011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[挖鼻]</td>\n",
       "      <td>0</td>\n",
       "      <td>这种级别的车也敢这样搞，钥匙一读或者车架号一给啥都出来了</td>\n",
       "      <td>0.144110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[握手]</td>\n",
       "      <td>1</td>\n",
       "      <td>对号//:转走//://:转发微博</td>\n",
       "      <td>0.444802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[玫瑰]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.679352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[鲜花]</td>\n",
       "      <td>1</td>\n",
       "      <td>，好喜欢你的文章《健身运动的八个误区》，忍不住打赏0.02元！→_→</td>\n",
       "      <td>0.423085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[笑cry]</td>\n",
       "      <td>0</td>\n",
       "      <td>我愣没有看出破绽！</td>\n",
       "      <td>0.180728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[给力]</td>\n",
       "      <td>1</td>\n",
       "      <td>支持👉</td>\n",
       "      <td>0.884524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[怒]</td>\n",
       "      <td>0</td>\n",
       "      <td>郑州，天哪！不敢看！邪恶在继续！</td>\n",
       "      <td>0.040671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[悲伤]</td>\n",
       "      <td>0</td>\n",
       "      <td>//:难道不怕报应吗医学院学生本是为了救人的，怎么狗狗的生命不是生命吗[伤心][伤心][伤心]</td>\n",
       "      <td>0.001836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[馋嘴]</td>\n",
       "      <td>1</td>\n",
       "      <td>说实话从来没吃过这玩意儿，我要买点尝尝</td>\n",
       "      <td>0.365903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[生病]</td>\n",
       "      <td>0</td>\n",
       "      <td>晚安！//:普京开赌了这赌场还不如中国自己开呢，肥水还能流到自己田里//:这海参崴口音玛德笑...</td>\n",
       "      <td>0.083048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[太开心]</td>\n",
       "      <td>1</td>\n",
       "      <td>4个月大的小北鼻第一次学滑雪，踩着爸爸量身定做的小滑板，成功滑了一小段之后咯咯直笑，萌翻了！</td>\n",
       "      <td>0.919914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[蛋糕]</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>0.674177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[可爱]</td>\n",
       "      <td>1</td>\n",
       "      <td>//:周末马住，戳图更精彩</td>\n",
       "      <td>0.877392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[抓狂]</td>\n",
       "      <td>0</td>\n",
       "      <td>只转不评！</td>\n",
       "      <td>0.023296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[哼]</td>\n",
       "      <td>0</td>\n",
       "      <td>//:……//:看链接才知道右边为啥骂…//:你们他妈的还用中文发微博干嘛，狗杂种</td>\n",
       "      <td>0.036602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[亲亲]</td>\n",
       "      <td>1</td>\n",
       "      <td>我们的红包是连发三天，每天一万现金加六十六本新书《天火大道》第一册，新书全部由我们自己网店炫...</td>\n",
       "      <td>0.712262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[礼物]</td>\n",
       "      <td>1</td>\n",
       "      <td>【活动·颁奖】开奖时间到！恭喜获得gala亲笔签名套装cd(youngforyou+追梦痴子...</td>\n",
       "      <td>0.997781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[赞啊]</td>\n",
       "      <td>1</td>\n",
       "      <td>分享西藏林芝的桃花美图</td>\n",
       "      <td>0.977206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[太阳]</td>\n",
       "      <td>1</td>\n",
       "      <td>每天给自己一个希望</td>\n",
       "      <td>0.931183</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion  emotion_label                                     input_sentence  \\\n",
       "0   [good]              1                                            //:转发微博   \n",
       "1     [哈哈]              1      前呼后拥的土豪也不少啊。。。//:我只敬重实干家！大忽悠则鄙视！//：哈哈哈，但总也追星。   \n",
       "2     [爱你]              1                                                      \n",
       "3     [嘻嘻]              1                                                      \n",
       "4      [衰]              0  郑州：这个辖区报警后几乎不管，后来，找了保险公司，通过gps定位，通过购车辖区民警，找到车。...   \n",
       "5      [心]              1                                              在剑川过年   \n",
       "6   [doge]              0  看懂了呀，譬如这句“诚邀社会各界贤达暨海内外校友沥临盛举，共襄伟业，再创辉煌，特此公告”这两...   \n",
       "7     [威武]              1             风华正茂之际，参军报国之时。中国军视网独家首曝中国武警部队2015年宣传片。   \n",
       "8      [泪]              0  速度//://:下载来看！//://://://://:马下载。//:马//://:转给刚起...   \n",
       "9      [汗]              0                                     屁股不错//:屁股这么复古啊   \n",
       "10    [偷乐]              0         小时候就知道怎么讨女孩子欢心了//:呀呀的，小时候凭着这门手艺，连校花都追着喊我哥。   \n",
       "11    [挤眼]              1  达拉斯转机去newark，看到有穿着疑似你阿国家队工作人员[傻眼]另外感觉今天跟我一样从sf...   \n",
       "12    [鼓掌]              1                                 //://:纯干货，实用性很强！！！   \n",
       "13    [拜拜]              0  省领导高度重视亲历事故现场，在第三天作出重要指示，要求要严肃追查事故原因，追究责任人，一定把...   \n",
       "14     [酷]              1  【郎仙寨全体当家倾巢出动，为了更好地劫道而努力训练】二当家第一次正规训练，呼吸和步伐配合尚可...   \n",
       "15    [阴险]              0                                         小伙伴只会强“抽”你   \n",
       "16     [色]              1  藏有美女//:荣耀中国红已经到港，找我预购的联系两瓶起，多多益善！想代理的也联系大慧，要是帅...   \n",
       "17   [笑哈哈]              1                             爸爸和女儿用咏春对打，最后老爸得手~超有爱~   \n",
       "18    [喵喵]              0                         lcdv-40662，鷹羽澪的写真作品。//:老司机   \n",
       "19    [吃惊]              0  我真的得喝茶静静静完好好学习哈//:普通的书都对你太简单了//:为啥呢？//:求推荐几本看看...   \n",
       "20     [赞]              1                                                      \n",
       "21     [耶]              1                    xoom一下，让快乐传递到至爱手里。支持你们举行好活动转走了！   \n",
       "22    [挖鼻]              0                       这种级别的车也敢这样搞，钥匙一读或者车架号一给啥都出来了   \n",
       "23    [握手]              1                                  对号//:转走//://:转发微博   \n",
       "24    [玫瑰]              1                                                      \n",
       "25    [鲜花]              1                 ，好喜欢你的文章《健身运动的八个误区》，忍不住打赏0.02元！→_→   \n",
       "26  [笑cry]              0                                          我愣没有看出破绽！   \n",
       "27    [给力]              1                                                支持👉   \n",
       "28     [怒]              0                                   郑州，天哪！不敢看！邪恶在继续！   \n",
       "29    [悲伤]              0    //:难道不怕报应吗医学院学生本是为了救人的，怎么狗狗的生命不是生命吗[伤心][伤心][伤心]   \n",
       "30    [馋嘴]              1                                说实话从来没吃过这玩意儿，我要买点尝尝   \n",
       "31    [生病]              0  晚安！//:普京开赌了这赌场还不如中国自己开呢，肥水还能流到自己田里//:这海参崴口音玛德笑...   \n",
       "32   [太开心]              1     4个月大的小北鼻第一次学滑雪，踩着爸爸量身定做的小滑板，成功滑了一小段之后咯咯直笑，萌翻了！   \n",
       "33    [蛋糕]              1                                                      \n",
       "34    [可爱]              1                                      //:周末马住，戳图更精彩   \n",
       "35    [抓狂]              0                                              只转不评！   \n",
       "36     [哼]              0          //:……//:看链接才知道右边为啥骂…//:你们他妈的还用中文发微博干嘛，狗杂种   \n",
       "37    [亲亲]              1  我们的红包是连发三天，每天一万现金加六十六本新书《天火大道》第一册，新书全部由我们自己网店炫...   \n",
       "38    [礼物]              1  【活动·颁奖】开奖时间到！恭喜获得gala亲笔签名套装cd(youngforyou+追梦痴子...   \n",
       "39    [赞啊]              1                                        分享西藏林芝的桃花美图   \n",
       "40    [太阳]              1                                          每天给自己一个希望   \n",
       "\n",
       "       score  \n",
       "0   0.790127  \n",
       "1   0.716519  \n",
       "2   0.699765  \n",
       "3   0.726079  \n",
       "4   0.739960  \n",
       "5   0.217364  \n",
       "6   0.578732  \n",
       "7   0.960041  \n",
       "8   0.386916  \n",
       "9   0.485089  \n",
       "10  0.455398  \n",
       "11  0.033581  \n",
       "12  0.939230  \n",
       "13  0.068516  \n",
       "14  0.971366  \n",
       "15  0.299891  \n",
       "16  0.808082  \n",
       "17  0.988662  \n",
       "18  0.351186  \n",
       "19  0.786918  \n",
       "20  0.756666  \n",
       "21  0.986011  \n",
       "22  0.144110  \n",
       "23  0.444802  \n",
       "24  0.679352  \n",
       "25  0.423085  \n",
       "26  0.180728  \n",
       "27  0.884524  \n",
       "28  0.040671  \n",
       "29  0.001836  \n",
       "30  0.365903  \n",
       "31  0.083048  \n",
       "32  0.919914  \n",
       "33  0.674177  \n",
       "34  0.877392  \n",
       "35  0.023296  \n",
       "36  0.036602  \n",
       "37  0.712262  \n",
       "38  0.997781  \n",
       "39  0.977206  \n",
       "40  0.931183  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'emotion':label_str_list,\n",
    "              'emotion_label': label_list,\n",
    "              'score': score_list,\n",
    "              'input_sentence': sent_list})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
