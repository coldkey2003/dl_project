{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from past.builtins import xrange\n",
    "\n",
    "from data import read_data\n",
    "import sys\n",
    "sys.path.append('../tfmodels')\n",
    "from sequential_model import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 929589 words from data/ptb.train.txt\n",
      "Read 73760 words from data/ptb.valid.txt\n",
      "Read 82430 words from data/ptb.test.txt\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "checkpoint_dir = 'checkpoints'\n",
    "data_name = 'ptb'\n",
    "\n",
    "count = []\n",
    "word2idx = {}\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "train_data = read_data('%s/%s.train.txt' % (data_dir, data_name), count, word2idx)\n",
    "valid_data = read_data('%s/%s.valid.txt' % (data_dir, data_name), count, word2idx)\n",
    "test_data = read_data('%s/%s.test.txt' % (data_dir, data_name), count, word2idx)\n",
    "\n",
    "idx2word = dict(zip(word2idx.values(), word2idx.keys()))\n",
    "nwords = len(word2idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "train_samples = len(range(0, len(train_data) - maxlen, 3))\n",
    "valid_samples = len(range(0, len(valid_data) - maxlen, 3))\n",
    "samples = train_samples+valid_samples\n",
    "idxs = np.arange(0, samples)\n",
    "train_idxs = idxs[0:train_samples]\n",
    "test_idxs = idxs[train_samples:]\n",
    "previous_words = np.empty(shape=(samples, maxlen), dtype=np.int32)\n",
    "next_words = np.empty(shape=(samples), dtype=np.int32)\n",
    "global_step = 0\n",
    "\n",
    "for i in range(0, len(train_data) - maxlen, 3):\n",
    "    previous_words[global_step] = train_data[i: i + maxlen]\n",
    "    next_words[global_step] = train_data[i + maxlen]\n",
    "    global_step += 1\n",
    "\n",
    "for i in range(0, len(valid_data) - maxlen, 3):\n",
    "    previous_words[global_step] = valid_data[i: i + maxlen]\n",
    "    next_words[global_step] = valid_data[i + maxlen]\n",
    "    global_step += 1\n",
    "\n",
    "seq_len = np.zeros(shape=(samples), dtype=np.int32) + maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_integer(\"nb_words\", nwords, \"term number in input sequence(zero mask) [20001]\")\n",
    "flags.DEFINE_integer(\"maxlen\", maxlen, \"the max length of input sequence [80]\")\n",
    "flags.DEFINE_integer(\"num_layers\", 1, \"the number of rnn layers [1]\")\n",
    "flags.DEFINE_integer(\"init_std\", 0.05, \"init_std\")\n",
    "flags.DEFINE_integer(\"init_scale\", 1, \"init_scale\")\n",
    "flags.DEFINE_integer(\"embedding_size\", 100, \"word embedding size [50]\")\n",
    "flags.DEFINE_integer(\"hidden_size\", 128, \"rnn hidden size [128]\")\n",
    "flags.DEFINE_float(\"keep_prob\", 0.9, \"keep probability of drop out [0.9]\")\n",
    "flags.DEFINE_float(\"learning_rate\", 0.002, \"learning rate [0.001]\")\n",
    "flags.DEFINE_integer(\"batch_size\", 512, \"batch size to use during training [128]\")\n",
    "flags.DEFINE_float(\"clip_gradients\", 5.0, \"clip gradients to this norm [5.0]\")\n",
    "flags.DEFINE_integer(\"n_epochs\", 1, \"number of epoch to use during training [10]\")\n",
    "flags.DEFINE_boolean(\"epoch_save\", True, \"save checkpoint or not in each epoch [True]\")\n",
    "flags.DEFINE_integer(\"print_step\", 100, \"print step duraing training [100]\")\n",
    "flags.DEFINE_string(\"logs_dir\", \"logs/\", \"logs directory [logs/]\")\n",
    "flags.DEFINE_string(\"model_dir\", \"model/\", \"model directory [model/]\")\n",
    "flags.DEFINE_boolean(\"dir_clear\", False, \"clear the log and model directory\")\n",
    "flags.DEFINE_boolean(\"lr_annealing\", False, \"use lr annealing or not after each epoch [False]\")\n",
    "flags.DEFINE_string(\"current_task_name\", 'url_self_prediction', \"current task name [self_prediction]\")\n",
    "flags.DEFINE_integer(\"gpu_id\", 0, \"default gpu id [0]\")\n",
    "flags.DEFINE_integer(\"gpu_num\", 4, \"gpu_num\")\n",
    "\n",
    "FLAGS = flags.FLAGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class SequentialModel(RNNLMModel):\n",
    "    def __init__(self, config, sess, current_task_name='sequence_model'):\n",
    "        super(SequentialModel, self).__init__(config, sess)\n",
    "    \n",
    "    def build_single_prediction(self, gpu_id=0, accK=5, nb_class=None):\n",
    "        self.params_1 = None\n",
    "        if nb_class is None:\n",
    "            nb_class = self.nb_words\n",
    "        with get_new_variable_scope('prediction') as pred_scope:    \n",
    "            prediction = my_full_connected(self.output_list[gpu_id][0][-1], nb_class, \n",
    "                                       add_bias=True, act=tf.identity, init_std=self.init_std)\n",
    "            self.tower_prediction_results.append(tf.nn.softmax(prediction))\n",
    "        with tf.name_scope('loss'): \n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.split_label[gpu_id], \n",
    "                                                                  logits=prediction)\n",
    "\n",
    "            self.params_1 = [param for param in self.input_params]\n",
    "            self.params_1.extend(tf.trainable_variables()[-2:])\n",
    "            grads, capped_gvs = my_compute_grad(self.opt, loss, self.params_1, \n",
    "                                                clip_type = 'clip_norm', \n",
    "                                                max_clip_grad=self.clip_gradients)            \n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.to_float(tf.nn.in_top_k(prediction, self.split_label[gpu_id],k=accK))        \n",
    "        self.__add_to_tower_list__(grads, capped_gvs, loss, accuracy, 'single')\n",
    "    \n",
    "    def build_single_output(self):\n",
    "        with tf.name_scope('output'):\n",
    "            label = tf.placeholder(tf.int64, [None], name=\"label\")\n",
    "            self.__add_to_graph_input__([label])\n",
    "            self.split_label = tf.split(label, self.gpu_num, 0)\n",
    "    \n",
    "    def build_output(self, type='self'):\n",
    "        if isinstance(type, list):\n",
    "            super(SequentialModel, self).build_output(type[0])\n",
    "            self.build_single_output()\n",
    "        else:\n",
    "            if type == 'single':\n",
    "                self.build_single_output()\n",
    "            else:\n",
    "                super(SequentialModel, self).build_output(type)\n",
    "    \n",
    "    def split_parameter(self, param):\n",
    "        if isinstance(param, list):\n",
    "            if len(param) > 1:\n",
    "                return param[0], param[1]\n",
    "            else:\n",
    "                return param[0], param[0]\n",
    "        else:\n",
    "            return param, param\n",
    "    \n",
    "    \n",
    "    def build_model(self, type=['self','single'], accK=5, nb_class=None):\n",
    "        self.build_input()\n",
    "        self.build_output(type)\n",
    "        accK1, accK2 = self.split_parameter(accK)\n",
    "        nb_class1, nb_class2 = self.split_parameter(nb_class)\n",
    "        new_type = type[0] if isinstance(type,list) else type\n",
    "        for idx, gpu_id in enumerate(self.gpus):\n",
    "            with tf.device('/gpu:%d' % gpu_id):\n",
    "                with tf.name_scope('Tower_%d' % (gpu_id)) as tower_scope:\n",
    "                    gpu_scope = tf.variable_scope('gpu', reuse=(idx!=0))\n",
    "                    with gpu_scope as gpu_scope:\n",
    "                        self.build_input_sequence(gpu_id=idx)\n",
    "                        if isinstance(type, list):\n",
    "                            self.build_sequence_prediction(type=new_type,gpu_id=idx,accK=accK1,nb_class=nb_class1)\n",
    "                            self.build_single_prediction(gpu_id=idx,accK=accK2,nb_class=nb_class2)\n",
    "                        else:\n",
    "                            if type == 'single':\n",
    "                                self.build_single_prediction(gpu_id=idx,accK=accK2,nb_class=nb_class2)\n",
    "                            else:\n",
    "                                self.build_sequence_prediction(type=new_type,gpu_id=idx,accK=accK1,nb_class=nb_class1)\n",
    "        self.build_model_aggregation()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable_name</th>\n",
       "      <th>variable_shape</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>global/Variable:0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpu/embedding/embedding_layer/embedding_table:0</td>\n",
       "      <td>[10000, 100]</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...</td>\n",
       "      <td>[228, 512]</td>\n",
       "      <td>116736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...</td>\n",
       "      <td>[512]</td>\n",
       "      <td>512.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...</td>\n",
       "      <td>[128]</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...</td>\n",
       "      <td>[128]</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...</td>\n",
       "      <td>[128]</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>gpu/prediction/conv_1d/he_uniform/W:0</td>\n",
       "      <td>[1, 128, 10000]</td>\n",
       "      <td>1280000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>gpu/prediction/conv_1d/B:0</td>\n",
       "      <td>[10000]</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>gpu/prediction_1/fully_connected/W:0</td>\n",
       "      <td>[128, 10000]</td>\n",
       "      <td>1280000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>gpu/prediction_1/fully_connected/B:0</td>\n",
       "      <td>[10000]</td>\n",
       "      <td>10000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        variable_name   variable_shape  \\\n",
       "0                                   global/Variable:0               []   \n",
       "1     gpu/embedding/embedding_layer/embedding_table:0     [10000, 100]   \n",
       "2   gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...       [228, 512]   \n",
       "3   gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...            [512]   \n",
       "4   gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...            [128]   \n",
       "5   gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...            [128]   \n",
       "6   gpu/rnn_lstm/rnn/multi_rnn_cell/cell_0/lstm_ce...            [128]   \n",
       "7               gpu/prediction/conv_1d/he_uniform/W:0  [1, 128, 10000]   \n",
       "8                          gpu/prediction/conv_1d/B:0          [10000]   \n",
       "9                gpu/prediction_1/fully_connected/W:0     [128, 10000]   \n",
       "10               gpu/prediction_1/fully_connected/B:0          [10000]   \n",
       "\n",
       "    parameters  \n",
       "0          1.0  \n",
       "1    1000000.0  \n",
       "2     116736.0  \n",
       "3        512.0  \n",
       "4        128.0  \n",
       "5        128.0  \n",
       "6        128.0  \n",
       "7    1280000.0  \n",
       "8      10000.0  \n",
       "9    1280000.0  \n",
       "10     10000.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 1, '... training ...')\n",
      "('Minibatch', 100, '/', 'loss:', 6.6084213)\n",
      "('Minibatch', 100, '/', 'accuracy:', 0.19540612)\n",
      "('Minibatch', 200, '/', 'loss:', 6.2081985)\n",
      "('Minibatch', 200, '/', 'accuracy:', 0.24480052)\n",
      "('Minibatch', 300, '/', 'loss:', 5.8365669)\n",
      "('Minibatch', 300, '/', 'accuracy:', 0.29508257)\n",
      "('Minibatch', 400, '/', 'loss:', 5.5646472)\n",
      "('Minibatch', 400, '/', 'accuracy:', 0.32933199)\n",
      "('Minibatch', 500, '/', 'loss:', 5.4794588)\n",
      "('Minibatch', 500, '/', 'accuracy:', 0.33192468)\n",
      "('Minibatch', 600, '/', 'loss:', 5.3142548)\n",
      "('Minibatch', 600, '/', 'accuracy:', 0.35432622)\n",
      "('epoch time:', 10.439584799607594)\n",
      "('Epoch', 1, 'training accuracy:', 0.27743490408295951)\n",
      "('Epoch', 1, '... test ...')\n",
      "('Epoch', 1, 'test accuracy:', 0.34220713674148157)\n",
      "Model saved in file: model/rnnlm.ckpt\n",
      "{'valid_los': 5.5011547202976958, 'loss': 5.9858551513327329, 'valid_perplexity': 244.97464620143543, 'learning_rate': 0.002, 'best_accuracy': 0.27743490408295951, 'epoch': 0, 'best_test_accuracy': 0.34220713674148157}\n",
      "('Epoch', 1, '... training ...')\n",
      "('Minibatch', 100, '/', 'loss:', 5.9812336)\n",
      "('Minibatch', 100, '/', 'accuracy:', 0.29101562)\n",
      "('Minibatch', 200, '/', 'loss:', 5.575902)\n",
      "('Minibatch', 200, '/', 'accuracy:', 0.32617188)\n",
      "('Minibatch', 300, '/', 'loss:', 5.2883639)\n",
      "('Minibatch', 300, '/', 'accuracy:', 0.38476562)\n",
      "('Minibatch', 400, '/', 'loss:', 5.4120979)\n",
      "('Minibatch', 400, '/', 'accuracy:', 0.35351562)\n",
      "('Minibatch', 500, '/', 'loss:', 5.3289528)\n",
      "('Minibatch', 500, '/', 'accuracy:', 0.3984375)\n",
      "('Minibatch', 600, '/', 'loss:', 5.1263952)\n",
      "('Minibatch', 600, '/', 'accuracy:', 0.4140625)\n",
      "('epoch time:', 2.6461299856503806)\n",
      "('Epoch', 1, 'training accuracy:', 0.34428134319687054)\n",
      "('Epoch', 1, '... test ...')\n",
      "('Epoch', 1, 'test accuracy:', 0.36603942652329752)\n",
      "Model saved in file: model/rnnlm.ckpt\n",
      "{'valid_los': 5.438054944607364, 'loss': 5.6850454122932721, 'valid_perplexity': 229.99439627562452, 'learning_rate': 0.002, 'best_accuracy': 0.34428134319687054, 'epoch': 0, 'best_test_accuracy': 0.36603942652329752}\n"
     ]
    }
   ],
   "source": [
    "graph_to_use = tf.Graph()\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "config.gpu_options.allow_growth=True\n",
    "with tf.Session(graph=graph_to_use, config=config) as session:\n",
    "    rnnlm_model = SequentialModel(FLAGS, session, current_task_name='seq_model')\n",
    "    rnnlm_model.build_model(type=['self','single'])\n",
    "    rnnlm_model.build_model_summary()\n",
    "    #rnnlm_model.model_restore()\n",
    "    display(rnnlm_model.model_summary())\n",
    "    rnnlm_model.run([previous_words,seq_len,next_words], train_idxs, test_idxs, run_type='self')\n",
    "    rnnlm_model.run([previous_words,seq_len,next_words], train_idxs, test_idxs, run_type='single')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
